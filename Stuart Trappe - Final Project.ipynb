{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cb06b4-f58d-46e0-8f71-b616bcc0512d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Project\n",
    "\n",
    "**Group HOMEWORK**. This final project can be collaborative. The maximum members of a group is 2. You can also work by yourself. Please respect the academic integrity. **Remember: if you get caught on cheating, you get F.**\n",
    "\n",
    "## A Introduction to the competition\n",
    "\n",
    "<img src=\"news-sexisme-EN.jpg\" alt=\"drawing\" width=\"380\"/>\n",
    "\n",
    "Sexism is a growing problem online. It can inflict harm on women who are targeted, make online spaces inaccessible and unwelcoming, and perpetuate social asymmetries and injustices. Automated tools are now widely deployed to find, and assess sexist content at scale but most only give classifications for generic, high-level categories, with no further explanation. Flagging what is sexist content and also explaining why it is sexist improves interpretability, trust and understanding of the decisions that automated tools use, empowering both users and moderators.\n",
    "\n",
    "This project is based on SemEval 2023 - Task 10 - Explainable Detection of Online Sexism (EDOS). [Here](https://codalab.lisn.upsaclay.fr/competitions/7124#learn_the_details-overview) you can find a detailed introduction to this task.\n",
    "\n",
    "You only need to complete **TASK A - Binary Sexism Detection: a two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist**. To cut down training time, we only use a subset of the original dataset (5k out of 20k). The dataset can be found in the same folder. \n",
    "\n",
    "Different from our previous homework, this competition gives you great flexibility (and very few hints), you can determine: \n",
    "-  how to preprocess the input text (e.g., remove emoji, remove stopwords, text lemmatization and stemming, etc.);\n",
    "-  which method to use to encode text features (e.g., TF-IDF, N-grams, Word2vec, GloVe, Part-of-Speech (POS), etc.);\n",
    "-  which model to use.\n",
    "\n",
    "## Requirements\n",
    "-  **Input**: the text for each instance.\n",
    "-  **Output**: the binary label for each instance.\n",
    "-  **Feature engineering**: use at least 2 different methods to extract features and encode text into numerical values.\n",
    "-  **Model selection**: implement with at least 3 different models and compare their performance.\n",
    "-  **Evaluation**: create a dataframe with rows indicating feature+model and columns indicating Precision, Accuracy and F1-score (using weighted average). Your results should have at least 6 rows (2 feature engineering methods x 3 models). Report best performance with (1) your feature engineering method, and (2) the model you choose. \n",
    "- **Format**: add explainations for each step (you can add markdown cells). At the end of the report, write a summary and answer the following questions: \n",
    "    - What preprocessing steps do you follow?\n",
    "    - How do you select the features from the inputs? \n",
    "    - Which model you use and what is the structure of your model?\n",
    "    - How do you train your model?\n",
    "    - What is the performance of your best model?\n",
    "    - What other models or feature engineering methods would you like to implement in the future?\n",
    "- **Two Rules**, violations will result in 0 points in the grade: \n",
    "    - Not allowed to use test set in the training: You CANNOT use any of the instances from test set in the training process. \n",
    "    - Not allowed to use code from generative AI (e.g., ChatGPT). \n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The performance should be only evaluated on the test set (a total of 1086 instances). Please split original dataset into train set and test set. The test set should NEVER be used in the training process. The evaluation metric is a combination of precision, recall, and f1-score (use `classification_report` in sklearn). \n",
    "\n",
    "The total points are 10.0. Each team will compete with other teams in the class on their best performance. Points will be deducted if not following the requirements above.\n",
    "\n",
    "If ALL the requirements are met:\n",
    "- Top 25\\% teams: 10.0 points.\n",
    "- Top 25\\% - 50\\% teams: 8.5 points.\n",
    "- Top 50\\% - 75\\% teams: 7.0 points.\n",
    "- Top 75\\% - 100\\% teams: 6.0 points.\n",
    "\n",
    "## Submission\n",
    "Similar as homework, submit both a PDF and .ipynb version of the report. \n",
    "\n",
    "The report should include: (a)code, (b)outputs, (c)explainations for each step, and (d)summary (you can add markdown cells). \n",
    "\n",
    "The due date is **December 8, Friday by 11:59pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37413433-63fe-4d5e-9681-8ea0ffca4686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mObject \"install\" is unknown, try \"ip help\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!ip install -U scikit-learn\n",
    "!pip install -U scikit-learn\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423397e-51c7-4358-87db-fcc31bb8b401",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "\n",
    "Here we are taking out data out of the CSV and into data frames. We transform the data by encoding the label column and separating it into two training and test data sets. From there, we create four data frames total for X and Y of the training and test set. To process our data before we can run feature selection or apply it to a model, we have to clean it up and making easier to read in. I started by opting to remove stop words, these are common words that are insignificant. From there, emails, numbers, html tags, special characters, and punctuation are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b7caf28-1c77-4f87-8a62-7f3a7f79e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewire_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sexism2022_english-9609</td>\n",
       "      <td>nigeria rape woman men rape back nsfw in niger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexism2022_english-16993</td>\n",
       "      <td>then keeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sexism2022_english-13149</td>\n",
       "      <td>like metallica video poor mutilated bastard sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sexism2022_english-13021</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sexism2022_english-966</td>\n",
       "      <td>bet wished gun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rewire_id                                               text\n",
       "0   sexism2022_english-9609  nigeria rape woman men rape back nsfw in niger...\n",
       "1  sexism2022_english-16993                                       then keeper \n",
       "2  sexism2022_english-13149  like metallica video poor mutilated bastard sa...\n",
       "3  sexism2022_english-13021                                              woman\n",
       "4    sexism2022_english-966                                     bet wished gun"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')  # Used once to download the stopwords\n",
    "\n",
    "df = pd.read_csv(\"edos_labelled_data.csv\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label']) # Encode label column, 1 is sexist, 0 is not sexist content\n",
    "\n",
    "# Put data into train and test datasets\n",
    "df_train, df_test = df[df[\"split\"]==\"train\"], df[df[\"split\"]==\"test\"]\n",
    "df_train, df_test = df_train.drop(columns=[\"split\"]), df_test.drop(columns=[\"split\"])\n",
    "\n",
    "# Put features and results in separate data frames\n",
    "features = ['rewire_id', 'text']\n",
    "X_train, Y_train = df_train[features], df_train['label']\n",
    "X_test, Y_test = df_test[features], df_test['label']\n",
    "\n",
    "# Here's where we start doing the data preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase all the data\n",
    "\n",
    "    # Get rid of words that aren't useful\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Remove emails, numbers, html tags, special characters, and punctuation\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove emails\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and punctuations\n",
    "\n",
    "    return text  # Get back sanitized, processed text\n",
    "\n",
    "# I got an error when running this so, I disabled the SettingWithCopyWarning for the specified lines\n",
    "pd.options.mode.chained_assignment = None\n",
    "X_train['text'] = X_train['text'].apply(preprocess_text)\n",
    "X_test['text'] = X_test['text'].apply(preprocess_text)\n",
    "\n",
    "pd.options.mode.chained_assignment = 'warn'  # Changed back to give the warning again\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea1495-e1df-4587-b60f-8d1065f97530",
   "metadata": {},
   "source": [
    "# Random Forest, Support Vector Machine, and Naive Bayes with TF-IDF\n",
    "\n",
    "Here we apply a Term Frequency Inverse Document Frequency algorithim to transform our text into values that can be used for prediction. I chose the three models as they were relatively simple to create and I thought would give me a solid baseline to compare against both another feature selection method and more advanced models. We initalized the TF-IDF from Sci Kit along with pipelines that allow us to rapidly train our models. At the end you'll see the output of 3 classification reports with Random Forest having the highest macro F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dda61f3f-bf68-4724-b61b-1623669fa5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.80      0.97      0.88       789\n",
      "      sexist       0.83      0.37      0.51       297\n",
      "\n",
      "    accuracy                           0.81      1086\n",
      "   macro avg       0.82      0.67      0.70      1086\n",
      "weighted avg       0.81      0.81      0.78      1086\n",
      "\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.78      0.98      0.87       789\n",
      "      sexist       0.85      0.26      0.40       297\n",
      "\n",
      "    accuracy                           0.79      1086\n",
      "   macro avg       0.81      0.62      0.64      1086\n",
      "weighted avg       0.80      0.79      0.74      1086\n",
      "\n",
      "\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.74      1.00      0.85       789\n",
      "      sexist       0.93      0.05      0.09       297\n",
      "\n",
      "    accuracy                           0.74      1086\n",
      "   macro avg       0.83      0.52      0.47      1086\n",
      "weighted avg       0.79      0.74      0.64      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Intialize our vector from Sci Kit\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Make the pipelines for each of the models\n",
    "rf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "nb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Here's where we train them\n",
    "rf_pipeline.fit(X_train['text'], Y_train)\n",
    "svm_pipeline.fit(X_train['text'], Y_train)\n",
    "nb_pipeline.fit(X_train['text'], Y_train)\n",
    "\n",
    "# Evaluate each one on the test data\n",
    "rf_predictions = rf_pipeline.predict(X_test['text'])\n",
    "svm_predictions = svm_pipeline.predict(X_test['text'])\n",
    "nb_predictions = nb_pipeline.predict(X_test['text'])\n",
    "\n",
    "# Generate reports on how well each model performed on test data for identify if sexist or not\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(Y_test, rf_predictions, target_names=['not sexist', 'sexist']))\n",
    "\n",
    "print(\"\\nSVM Classification Report:\")\n",
    "print(classification_report(Y_test, svm_predictions, target_names=['not sexist', 'sexist']))\n",
    "\n",
    "print(\"\\nNaive Bayes Classification Report:\")\n",
    "print(classification_report(Y_test, nb_predictions, target_names=['not sexist', 'sexist']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1eb44-8725-43de-ad76-884fe6463cd0",
   "metadata": {},
   "source": [
    "# Random Forest, Support Vector Machine, and Naive Bayes with Word2Vec\n",
    "\n",
    "As an alternative feature selection methodology, I chose Word2Vec. It develops an understanding of word associations from large bodies of text and can understand more complex ideas like synonyms. To create it, I had to upload the multi gigabyte file of the pre-trained w2v model from Google. We go through each word in a given line of text and create an array of vectors. These are then used to train our classifiers and just as before, output three classifcation reports. Here we see a substancial improvement to our SVM, but it is still slightly behind our Random Forest with TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a194903f-9758-471a-b29c-f3b2d7a9e014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.75      0.98      0.85       789\n",
      "      sexist       0.71      0.14      0.24       297\n",
      "\n",
      "    accuracy                           0.75      1086\n",
      "   macro avg       0.73      0.56      0.54      1086\n",
      "weighted avg       0.74      0.75      0.68      1086\n",
      "\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.80      0.95      0.87       789\n",
      "      sexist       0.72      0.37      0.49       297\n",
      "\n",
      "    accuracy                           0.79      1086\n",
      "   macro avg       0.76      0.66      0.68      1086\n",
      "weighted avg       0.78      0.79      0.76      1086\n",
      "\n",
      "\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.84      0.50      0.63       789\n",
      "      sexist       0.36      0.75      0.49       297\n",
      "\n",
      "    accuracy                           0.57      1086\n",
      "   macro avg       0.60      0.63      0.56      1086\n",
      "weighted avg       0.71      0.57      0.59      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.naive_bayes import GaussianNB  # Changed because I had errors when using continuous features\n",
    "\n",
    "# Here's where we get the massive file that I got from Google\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Function to vectorize a list of texts\n",
    "def vectorize_texts(text_list):\n",
    "    vectors = []\n",
    "    for text in text_list:\n",
    "        words = text.split()\n",
    "        word_vectors_list = [word_vectors[word] for word in words if word in word_vectors]\n",
    "        if len(word_vectors_list) > 0:\n",
    "            vectors.append(np.mean(word_vectors_list, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(300))  # 300 is the dimensionality of the Word2Vec vectors\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Use that function on the training and testing data\n",
    "X_train_vect = vectorize_texts(X_train['text'])\n",
    "X_test_vect = vectorize_texts(X_test['text'])\n",
    "\n",
    "# Apply our 3 models to the training data\n",
    "rf_classifier = RandomForestClassifier().fit(X_train_vect, Y_train)\n",
    "svm_classifier = SVC().fit(X_train_vect, Y_train)\n",
    "nb_classifier = GaussianNB().fit(X_train_vect, Y_train)  # GaussianNB is used as it works with continuous features\n",
    "\n",
    "# Run the predictions\n",
    "rf_predictions = rf_classifier.predict(X_test_vect)\n",
    "svm_predictions = svm_classifier.predict(X_test_vect)\n",
    "nb_predictions = nb_classifier.predict(X_test_vect)\n",
    "\n",
    "# Output performance\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(Y_test, rf_predictions, target_names=['not sexist', 'sexist']))\n",
    "\n",
    "print(\"\\nSVM Classification Report:\")\n",
    "print(classification_report(Y_test, svm_predictions, target_names=['not sexist', 'sexist']))\n",
    "\n",
    "print(\"\\nNaive Bayes Classification Report:\")\n",
    "print(classification_report(Y_test, nb_predictions, target_names=['not sexist', 'sexist']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa0087-8cbb-4f6b-ba5d-c4b0f44163e5",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "Here we implement and fine tune a Bidirectional Encoder Representations from Transformers model. It's widely considered a baseline in natural language processing. We create tokenizer and model from the Hugging Face transformer library. After tokenizing, we use data loader to create iterable datasets with batch processing for more efficient training. Our optimizer is AdamW which will update model weights as we go through training. Of note here, are three parameters, max_length in the tokenizing process, batch_size in the data loader, and the number of epochs during training. I changed these based on [this](https://arxiv.org/pdf/2305.00076v1.pdf) report to see if my data preprocessing and fine tuning could match their results. It seemed the most optimized for data of this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "747f6842-412c-408f-a4e3-cda7f2337f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.87      0.85      0.86       789\n",
      "      sexist       0.62      0.67      0.65       297\n",
      "\n",
      "    accuracy                           0.80      1086\n",
      "   macro avg       0.75      0.76      0.75      1086\n",
      "weighted avg       0.81      0.80      0.80      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Get tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokinization of the texts\n",
    "tokenized_train = tokenizer(X_train['text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "tokenized_test = tokenizer(X_test['text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Training + testing set up with TensorDataset objs and DataLoader utility\n",
    "train_dataset = TensorDataset(tokenized_train['input_ids'], tokenized_train['attention_mask'], torch.tensor(Y_train.tolist()))\n",
    "test_dataset = TensorDataset(tokenized_test['input_ids'], tokenized_test['attention_mask'], torch.tensor(Y_test.tolist()))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Get params\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # I had to use cloud compute with a dGPU for this\n",
    "model.to(device)\n",
    "\n",
    "# Here's where we do training\n",
    "num_epochs = 20 # Number of times a dataset passes through an algorithm\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Eval\n",
    "model.eval()\n",
    "\n",
    "# Going to use the same loop for all the other models, just different training\n",
    "all_predictions = []  # Goes over the test data in batches, get output, and put predictions here\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_predictions, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8501f6-a435-4851-bd09-6c4b3fabbd85",
   "metadata": {},
   "source": [
    "# XLM-RoBERTa\n",
    "\n",
    "XLM-R is a pre-trained model that seemed to have massive performance on handling text processing and classification. Similar to before, we import a tokenizer and model, note that we specificy 2 labels here for binary classification. Like the other models, what we do here is tokenize the inputs, create TensorDataset and DataLoader for both the test train and test, and then train them with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a97aa148-82c3-4dd7-ad52-2bf8ad92e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.89      0.85      0.87       789\n",
      "      sexist       0.65      0.72      0.68       297\n",
      "\n",
      "    accuracy                           0.81      1086\n",
      "   macro avg       0.77      0.78      0.77      1086\n",
      "weighted avg       0.82      0.81      0.82      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Get tokenizer and model\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=2)\n",
    "\n",
    "# Tokenizization like before, same max_length params too\n",
    "tokenized_train = tokenizer.batch_encode_plus(X_train['text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "tokenized_test = tokenizer.batch_encode_plus(X_test['text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Training + testing set up\n",
    "train_dataset = TensorDataset(tokenized_train['input_ids'], tokenized_train['attention_mask'], torch.tensor(Y_train.tolist()))\n",
    "test_dataset = TensorDataset(tokenized_test['input_ids'], tokenized_test['attention_mask'], torch.tensor(Y_test.tolist()))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # Same batch sizes as before\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training setup\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)  # Intialize the optimizer with a learning rate \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Same deal as before, better to run on GPU\n",
    "model.to(device)\n",
    "\n",
    "# Begin training\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Eval\n",
    "model.eval()\n",
    "\n",
    "# Same loop and technique as with all the others\n",
    "all_predictions = []  # Goes over the test data in batches, get output, and put predictions here\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_predictions, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d4604-e491-4b1d-a817-a4fca22fc5a0",
   "metadata": {},
   "source": [
    "# HateBERT\n",
    "\n",
    "A bit of an odd-ball choice, but I found it online and it seemed perfectly fit to both the text size and the goal. It is specifically designed to detect abusive language. The process for setting it up and running it is nearly identical to the other models, we just have to tokenize, prep Datasets and Loaders, train, and evaluate. HateBERT out performed general BERT on the datasets they tried it with, so I was curious if I would get the same or a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e81f7d65-d064-4153-af0b-02bb1a47bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.87      0.91      0.89       789\n",
      "      sexist       0.73      0.62      0.67       297\n",
      "\n",
      "    accuracy                           0.83      1086\n",
      "   macro avg       0.80      0.77      0.78      1086\n",
      "weighted avg       0.83      0.83      0.83      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "# Get HateBERT specific tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('GroNLP/hateBERT')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('GroNLP/hateBERT', num_labels=2)\n",
    "\n",
    "# Tokenizization like before, same max_length params too\n",
    "tokenized_train = tokenizer.batch_encode_plus(X_train['text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "tokenized_test = tokenizer.batch_encode_plus(X_test['text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Training + testing set up\n",
    "train_dataset = TensorDataset(tokenized_train['input_ids'], tokenized_train['attention_mask'], torch.tensor(Y_train.tolist()))\n",
    "test_dataset = TensorDataset(tokenized_test['input_ids'], tokenized_test['attention_mask'], torch.tensor(Y_test.tolist()))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training setup (as before)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Very GPU intensive, takes a good bit of time to run\n",
    "model.to(device)\n",
    "\n",
    "# Train models again\n",
    "num_epochs = 20  # Same number of epochs before\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Eval\n",
    "model.eval()\n",
    "\n",
    "# Same loop and technique as with all the others\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():  # Goes over the test data in batches, get output, and put predictions here\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_predictions, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "afaa509d-11b7-432d-8f3f-8de5d46e7272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Feature and Model  Precision  Accuracy  F1-Score\n",
      "0    TF-IDF w/ Random Forest       0.81      0.81      0.78\n",
      "1              TF-IDF w/ SVM       0.80      0.79      0.74\n",
      "2      TF-IDF w/ Naive Bayes       0.79      0.74      0.64\n",
      "3  Word2Vec w/ Random Forest       0.74      0.75      0.68\n",
      "4            Word2Vec w/ SVM       0.78      0.79      0.76\n",
      "5    Word2Vec w/ Naive Bayes       0.71      0.57      0.59\n",
      "6                       BERT       0.81      0.80      0.80\n",
      "7                      XLM-R       0.82      0.81      0.82\n",
      "8                   HateBERT       0.83      0.83      0.83\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"Feature and Model\": [\"TF-IDF w/ Random Forest\", \"TF-IDF w/ SVM\", \"TF-IDF w/ Naive Bayes\",\n",
    "                      \"Word2Vec w/ Random Forest\", \"Word2Vec w/ SVM\", \"Word2Vec w/ Naive Bayes\",\n",
    "                     \"BERT\", \"XLM-R\", \"HateBERT\"],\n",
    "    \"Precision\": [0.81, 0.80, 0.79, 0.74, 0.78, 0.71, 0.81, 0.82, 0.83],  # Hypothetical precision scores\n",
    "    \"Accuracy\": [0.81, 0.79, 0.74, 0.75, 0.79, 0.57, 0.80, 0.81, 0.83],   # Hypothetical accuracy scores\n",
    "    \"F1-Score\": [0.78, 0.74, 0.64, 0.68, 0.76, 0.59, 0.80, 0.82, 0.83]    # Hypothetical F1 scores\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827b394-47a9-4ccc-8ee3-9a9059064cff",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. What preprocessing steps do you follow?\n",
    "   \n",
    "   Your answer: To begin with, I transformed the sexist column so that 1 would indicate if the content was sexist and 0 if not. From there, I split the data into training and test dataframes as well as into X and Y. I continued by opting to remove stop words, these are common words that are insignificant. I used the stop words list built into the Natural Language Toolkit. From there, I also removed emails, numbers, html tags, special characters, and punctuation\n",
    "   \n",
    "2. How do you select the features from the inputs?\n",
    "   \n",
    "   Your answer: I explored a number of feature selection methods. TF-IDF stood out to me for weighting the occurance of rarer words that, after a cursory glance at the CSV file, seems to correspond with counts of sexism. I also chose Word2Vec as I had heard it understood relationships between words and maintened context. We got some mixed results as TF-IDF seemed to make Random Forest perform very well and Word2Vec made our SVM the best model in the bunch for their respective runs.\n",
    "   \n",
    "3. Which model you use and what is the structure of your model?\n",
    "   \n",
    "   Your answer: I created 6 separate models, 3 to try 2 feature selection techniques, and another 3 to experiment with more advanced models. Random Forest, SVM, and Naive Bayes were ones we had all covered in class and explored before. The only significant change to them was the type of encoded data fed into them. The other 3 models are BERT, XLM-R, and HateBERT. I covered a bit of the reasons why I chose them above, but the main point was I wanted to see just how much more advanced some of the newer models are with the reference point of simpler classification methods. They are quite complex, but I tried to break down some of the higher level steps in my code.\n",
    "   \n",
    "4. How do you train your model?\n",
    "   \n",
    "   Your answer: We fed the data into models.  For the first 3 models, I used pipeline and the last 3, after tokenizing the inputs, I used batches with train_loader to simplify things. Those served as the basis of our model which we then used to evaluate on our test data.\n",
    "   \n",
    "5. What is the performance of your best model?\n",
    "   \n",
    "   Your answer: My best performing model is HateBERT with on both macro and weighted average F1 with scores of 0.78 and 0.83 respectively.\n",
    "   \n",
    "6. What other models or feature engineering methods would you like to implement in the future?\n",
    "   \n",
    "   Your answer: In the future, I'd like to devote more time to both data preprocessing (e.g. not filtering out some stop wards like 'she' as it may provide useful context) and feature selection. I only explored two and I'm sure there may be more useful ocnes.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd104c43-221b-4f30-b630-1696c1119a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
